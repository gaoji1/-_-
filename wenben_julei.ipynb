{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先把现病史都读进来看看\n",
    "#tf = (当前词出现的个数)/(所有词的总个数)\n",
    "#idf = log[(文段个数)/(包含当前词的文段个数+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "医学分词包 : ./wstool2.jar\n",
      "number of corpus : 200\n",
      "begin filling the similar matrix : 2018-07-03 15:40:09.202436\n",
      "end filling the similar matrix : 2018-07-03 15:40:11.956255\n",
      "shape of the similar matrix : (200, 200)\n",
      "每个类的内部平均间距 : [0.0, 4.2491873609703695e-18, 8.061726673912552e-17, 1.993045536433795e-17, 0.1318781189353395, 0.1426943107545973, 0.0, 3.1031676915590914e-17, 2.8609792490763984e-17, 0.07889476312259748, 1.734723475976807e-18, 2.7755575615628914e-17, 1.993045536433795e-17, 0.0, 0.10121984942591862, 0.0, 0.0, 0.0, 1.3877787807814457e-17, 5.485677294651096e-18, 5.551115123125783e-17, 5.551115123125783e-17, 0.133748274766272, 0.0, 0.09234661088362532, 0.19352998380474684, 0.0, 0.11120032080315027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11402424932953113, 1.734723475976807e-18, 0.12065431915491687, 0.12204193980430691, 0.12359498964015547, 3.496446946337351e-18, 0.0, 3.1080126089379415e-17, 0.0, 0.0, 0.0, 5.578154324147574e-17, 2.8609792490763984e-17, 2.8609792490763984e-17, 0.0, 0.057591326968765524, 6.938893903907228e-18, 0.0, 2.8609792490763984e-17, 0.0, 2.8609792490763984e-17, 2.8609792490763984e-17, 2.8609792490763984e-17, 2.8609792490763984e-17, 0.0, 0.0, 0.0, 0.0, 0.17181684605158115, 0.09370316141821732, 2.8609792490763984e-17, 0.0, 0.0, 1.3877787807814457e-17, 0.1396055659296384, 0.0, 1.734723475976807e-18, 0.0, 0.0, 0.0, 0.10816597228280074, 5.578154324147574e-17, 5.485677294651096e-18, 0.051554705669299955, 0.0, 0.0, 0.0, 9.813077866773595e-18, 0.0, 1.734723475976807e-18, 9.813077866773595e-18, 0.15575855190954527, 0.1533413218143716, 1.734723475976807e-18, 0.0, 5.485677294651096e-18]\n",
      "每个类包含的corpus个数 : [1, 1, 3, 1, 3, 8, 1, 1, 1, 11, 1, 1, 4, 1, 5, 1, 1, 1, 1, 1, 1, 1, 41, 1, 2, 6, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 10, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 1]\n",
      "每个类的内部距离方差 : [0.0, 2.6727647100921956e-51, -1.3684555315672042e-48, 0.0, 0.0007174759550729534, 0.00020245075278206917, 0.0, -1.7105694144590052e-49, 0.0, 0.00013976359399286205, 0.0, 0.0, 0.0, 0.0, 0.0013073633520605237, 0.0, 0.0, 0.0, 0.0, -5.345529420184391e-51, 0.0, 0.0, 0.004431842881089894, 0.0, -1.734723475976807e-18, 0.0009841680281247808, 0.0, 0.0002507817195314052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.983364658975431e-06, 0.0, 0.001012342985290328, 3.469446951953614e-18, 0.0004857689853642929, 2.6727647100921956e-51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.713467819338803e-05, 1.734723475976807e-18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013219837669862563, 0.0, -5.345529420184391e-51, 4.336808689942018e-19, 0.0, 0.0, 0.0, -2.1382117680737565e-50, 0.0, 0.0, -2.1382117680737565e-50, 9.855927274904017e-05, 0.0010401597769786712, 0.0, 0.0, -5.345529420184391e-51]\n",
      "每个类包含的corpus个数 : [1, 1, 3, 1, 3, 8, 1, 1, 1, 11, 1, 1, 4, 1, 5, 1, 1, 1, 1, 1, 1, 1, 41, 1, 2, 6, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 10, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 1]\n",
      "两类之间的距离最大为 : 3.185793508239954\n",
      "两类之间的距离最小为 : 0.12389924355395544\n",
      "两类之间的距离平均为 : 0.9320329368405686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9320329368405686"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 包导入\n",
    "import datetime\n",
    "import json\n",
    "import jpype\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans       #导入K-means算法包\n",
    "\n",
    "#初始化JAVA虚拟机函数\n",
    "def init_split_JVM():\n",
    "    \"\"\"初始化JVM，加载分词包，返回jvm\"\"\"\n",
    "    print(\"医学分词包\",\":\",'./wstool2.jar')\n",
    "    jpype.startJVM(jpype.getDefaultJVMPath(),\"-ea\", \"-Djava.class.path=%s\" % ('./wstool2.jar'))\n",
    "    #sys.exit()\n",
    "    jd = jpype.JClass(\"com.ifly.ti.ws2.WSEngineMgr\")  \n",
    "    jd.init(u'ws.cfg',u'UTF-8')\n",
    "    return jd\n",
    "\n",
    "def inner_mean_distance(corpus_vector,kmeans):\n",
    "    \"\"\"计算类内平均距离，输入为俩个变量：1.文段的向量表示 2.kmeans的聚类结果类变量\n",
    "       输出为一个列表result,其中result[i]表示第i类的类内平均距离\"\"\"\n",
    "    #聚类结果\n",
    "    cluster_list = kmeans.labels_\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    result = [0 for i in range(CLUSTER_NUM)]\n",
    "    for i,item in enumerate(corpus_vector):\n",
    "        #当前corpus属于哪个类\n",
    "        temp_class = cluster_list[i]\n",
    "        result[temp_class] += np.sqrt(np.sum(np.square(item - cluster_centers[temp_class])))\n",
    "    for i,item in enumerate(result):\n",
    "        result[i] = item/np.sum(cluster_list == i)\n",
    "    print(\"每个类的内部平均间距\",\":\",result)\n",
    "    print(\"每个类包含的corpus个数\",\":\",[np.sum(cluster_list==i) for i in range(CLUSTER_NUM)])\n",
    "    return result\n",
    "\n",
    "def inner_distance_variance(corpus_vector,kmeans):\n",
    "    \"\"\"计算每一类的点到类中心的距离的方差,输入1.corpus向量，2.kmeans结果向量\n",
    "       返回result为每一类的距离的方差\"\"\"\n",
    "    cluster_list = kmeans.labels_\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    means = [0 for i in range(CLUSTER_NUM)] #均值\n",
    "    mean_square = [0 for i in range(CLUSTER_NUM)] #均方值\n",
    "    result = [0 for i in range(CLUSTER_NUM)] #结果\n",
    "    for i,item in enumerate(corpus_vector):\n",
    "        #当前corpus属于哪个类\n",
    "        temp_class = cluster_list[i]\n",
    "        means[temp_class] += np.sqrt(np.sum(np.square(item - cluster_centers[temp_class])))\n",
    "        mean_square[temp_class] += np.sum(np.square(item - cluster_centers[temp_class]))\n",
    "    for i in range(len(result)):\n",
    "        means[i] = means[i]/np.sum(cluster_list == i)\n",
    "        mean_square[i] = mean_square[i]/np.sum(cluster_list == i)\n",
    "        result[i] = mean_square[i] - pow(means[i],2)\n",
    "#     print(\"每个类的内部距离均方值\",\":\",mean_square)\n",
    "    print(\"每个类的内部距离方差\",\":\",result)\n",
    "    print(\"每个类包含的corpus个数\",\":\",[np.sum(cluster_list==i) for i in range(CLUSTER_NUM)])\n",
    "    return result\n",
    "\n",
    "def label_distance(kmeans):\n",
    "    \"\"\"计算平均类间距离,输入为kmeans结果类,输出为平均类间距离result\"\"\"\n",
    "    distance_list = []\n",
    "    for i in range(len(kmeans.cluster_centers_)):\n",
    "        for j in range(len(kmeans.cluster_centers_)):\n",
    "            if j<=i:\n",
    "                continue\n",
    "            distance_list.append( np.sqrt(np.sum(np.square(kmeans.cluster_centers_[i] - kmeans.cluster_centers_[j]))) )\n",
    "    print(\"两类之间的距离最大为\",\":\",np.max(distance_list))\n",
    "    print(\"两类之间的距离最小为\",\":\",np.min(distance_list))\n",
    "    print(\"两类之间的距离平均为\",\":\",np.mean(distance_list))\n",
    "    return np.mean(distance_list)\n",
    "\n",
    "#具体逻辑\n",
    "def corpus_encoding(tf_idf_dic,corpus):\n",
    "    \"\"\"语段编码，输入两个参数分别是tf_idf字典跟语段corpus\"\"\"\n",
    "    code = []\n",
    "    total_sum = 0\n",
    "    for item in tf_idf_dic:\n",
    "        code.append(tf_idf_dic[item]*corpus.count(item))\n",
    "        total_sum += tf_idf_dic[item]*corpus.count(item)\n",
    "    if total_sum==0:\n",
    "        return code\n",
    "    for i,item in enumerate(code):\n",
    "        code[i] = item/total_sum\n",
    "    return code\n",
    "\n",
    "def add_dictionary(dic,item):\n",
    "    \"\"\"向字典中添加内容，如果内容已经在字典中，就+1，如果内同不在字典中，就初始化为1\"\"\"\n",
    "    if item in dic:\n",
    "        dic[item] += 1\n",
    "    else:\n",
    "        dic[item] = 1\n",
    "        \n",
    "def is_punctuation(split_item):\n",
    "    \"\"\"检测分词后的任意一项是不是一个标点符号\"\"\"\n",
    "    split_item = split_item.replace(\" \",\"\")\n",
    "    if split_item in \",.，。、:：“ ”\\\"\\\"()（） \":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def isJson(jsonstr):\n",
    "    \"\"\"判断是否是json\"\"\"\n",
    "    try:\n",
    "        a = json.loads(jsonstr)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def corpus_to_vector(number_of_corpus):         \n",
    "    #先初始化分词工具\n",
    "    split_tool = init_split_JVM()\n",
    "    #打开文件，计算tf-idf值，最终的目的是得到tf-idf值的字典\n",
    "    with open(\"huxi_result_break.json\",\"r\",encoding=\"utf8\") as load_f:\n",
    "        #词数量，统计每个词在语料中出现的次数\n",
    "        word_count = {}\n",
    "        #总共的词数量\n",
    "        total_count = 0\n",
    "        #每个词的词频:tf\n",
    "        word_frequency = {}\n",
    "        #总的语段个数\n",
    "        total_corpus = 0\n",
    "        #有目标词的语段，统计当前词在几个语段中出现\n",
    "        corpus_with_word = {}\n",
    "        #idf字典\n",
    "        idf_dic = {}\n",
    "        #tf-idf字典\n",
    "        tf_idf_dic = {}\n",
    "        for i in range(number_of_corpus):\n",
    "            line = load_f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if not isJson(line):\n",
    "                continue\n",
    "            #json转字符串\n",
    "            json_to_dic = json.loads(line)\n",
    "            #总语段数＋1\n",
    "            total_corpus += 1\n",
    "            #对现病史分词,去标点符号 ,去除长度为1的分词项\n",
    "            split_result = split_tool.process(json_to_dic[\"xianbingshi\"],4)\n",
    "            split_result = [item for item in split_result if not is_punctuation(item) and len(item)>1]\n",
    "            #在当前语段中统计词出现个数\n",
    "            for item in split_result:\n",
    "                total_count += 1\n",
    "                add_dictionary(word_count,item)\n",
    "            #在当前语段中统计词是否出现\n",
    "            for item in set(split_result):\n",
    "                add_dictionary(corpus_with_word,item)\n",
    "        #计算词频tf\n",
    "#         for item in word_count:\n",
    "#             word_frequency[item] = word_count[item]/total_count\n",
    "        #计算idf值\n",
    "        for item in corpus_with_word:\n",
    "            idf_dic[item] = math.log( total_corpus/(1+corpus_with_word[item]) )\n",
    "        #计算tf-idf\n",
    "#         for item in idf_dic:\n",
    "#             tf_idf_dic[item] = word_frequency[item] * idf_dic[item]\n",
    "        #tf-idf取值最大的前1000项\n",
    "        idf_dic = dict(sorted(idf_dic.items(),key = lambda x:x[1],reverse = True)[:1000])\n",
    "#         tf_idf_dic = dict(sorted(tf_idf_dic.items(),key = lambda x:x[1],reverse = True)[:1000])\n",
    "        load_f.close()\n",
    "\n",
    "    #打开文件，对于每一个corpus，计算它的编码    \n",
    "    with open(\"huxi_result_break.json\",\"r\",encoding=\"utf8\") as load_f:\n",
    "        #文本的编码\n",
    "        corpus_code = []\n",
    "        for i in range(number_of_corpus):\n",
    "            line = load_f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if not isJson(line):\n",
    "                continue\n",
    "            #json转字符串\n",
    "            json_to_dic = json.loads(line)\n",
    "            corpus_code.append(corpus_encoding(idf_dic,json_to_dic[\"xianbingshi\"]))\n",
    "        print(\"number of corpus\",\":\",len(corpus_code))\n",
    "        load_f.close()\n",
    "\n",
    "    #计算文本相似度矩阵\n",
    "    similar_matrix = np.zeros( (len(corpus_code),len(corpus_code)) )\n",
    "    print(\"begin filling the similar matrix\",\":\",datetime.datetime.now())\n",
    "    for i in range( len(corpus_code) ):\n",
    "        for j in range( len(corpus_code) ):\n",
    "            similar_matrix[i][j] = np.sum(np.array(corpus_code[i])*np.array(corpus_code[j]))\n",
    "    print(\"end filling the similar matrix\",\":\",datetime.datetime.now())\n",
    "    #相似度矩阵的每一行作为相应corpus的向量vector\n",
    "    print(\"shape of the similar matrix\",\":\",similar_matrix.shape)\n",
    "    return similar_matrix\n",
    "    jpype.shutdownJVM()\n",
    "\n",
    "#常量定义\n",
    "DATA_NUM = 200#需要使用的数据数\n",
    "CLUSTER_NUM = 90#聚类的数量\n",
    "\n",
    "#文段转向量\n",
    "corpus_vector = corpus_to_vector(DATA_NUM)\n",
    "\n",
    "#实行kmeans算法\n",
    "kmeans = KMeans(n_clusters=CLUSTER_NUM, random_state=0).fit(corpus_vector)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)  #全部输出\n",
    "#把文件中的相应句子根据聚类结果输出\n",
    "with open(\"huxi_result_break.json\",\"r\",encoding=\"utf8\") as load_f:\n",
    "    \"\"\"把句子分门别类存放\"\"\"\n",
    "    corpus_group = [[]for i in range(CLUSTER_NUM)]\n",
    "    for i in range(len(corpus_vector)):\n",
    "        line = load_f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "#       每类所有句子输出\n",
    "        corpus_group[kmeans.labels_[i]].append(line)\n",
    "#       每类出一个句子\n",
    "#         if len(corpus_group[kmeans.labels_[i]]) == 0:\n",
    "#             corpus_group[kmeans.labels_[i]].append(line)\n",
    "with open(\"temp.json\",\"w\",encoding=\"utf8\") as write_f:\n",
    "    \"\"\"写入文件temp.json\"\"\"\n",
    "    for i,items in enumerate(corpus_group):\n",
    "        write_f.write(\"=\"*100+\"\\n\")\n",
    "        write_f.write(\"class\"+str(i)+\"\\n\")\n",
    "        for j,item in enumerate(corpus_group[i]):\n",
    "            json_to_dic = json.loads(item)\n",
    "            write_f.write(json_to_dic[\"xianbingshi\"]+\"\\n\")\n",
    "            write_f.write(\"\\n\")\n",
    "\n",
    "\n",
    "inner_mean_distance(corpus_vector,kmeans)\n",
    "inner_distance_variance(corpus_vector,kmeans)\n",
    "label_distance(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"abc\".count(\"p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_group = [[] for i in range(8)]\n",
    "corpus_group[0].append(1)\n",
    "corpus_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4005"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "45*89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025563136039013133"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([1.1020129768715412e-18, 0.022980885120420468, 0.013968837876394401, 0.015323806240162824, 0.01936731997417723, 0.022867476265922084, 3.718755171350658e-18, 0.018858653800309155, 0.019167600622650247, 7.472347801331162e-19, 0.021044704459492823, 0.016795309155659348, 0.021359145577586128, 0.01786865573665121, 1.3145951341386741e-18, 1.5592475509958832e-18, 2.001740834465209e-18, 5.819701966859399e-19, 1.0539649716793541e-18, 7.047314121155779e-19, 0.01911245841886791, 1.2329169036399452e-18, 5.543298387644657e-19, 0.015550627860775516, 1.4893883980072198e-18, 1.1672916146702327e-18, 8.926168072956793e-19, 1.3891836150286352e-18, 1.4419914368709812e-18, 0.025563136039013133, 1.0301926573946376e-18, 0.016868997115931962, 0.018922801371813434, 0.0215480910809861, 1.0174329032879155e-18, 3.520542838315659e-18, 0.016924018416315637, 1.6085314012338225e-18, 2.004674864379183e-18, 0.01466436418392951, 0.019485610955873756, 1.2513173895913659e-18, 8.026941142455614e-19, 1.1833459533953078e-18, 0.023238490974757216, 9.57853080783484e-19, 0.020051312701609193, 4.0567130674367425e-19, 6.399893068361695e-19, 0.015432636469860383, 1.1617290783731975e-18, 1.1034287507866773e-18, 0.014047765909110586, 1.2040411713683422e-18, 1.1250230255641147e-18, 0.019728724333468933, 2.128480372581815e-18, 1.2794045034780968e-18, 5.260340572254303e-19, 9.720100742653923e-19, 8.713758558935813e-19, 4.268508221585787e-19, 1.3789529887332297e-18, 1.1261652640451394e-18, 1.2990646933079378e-18, 3.5547945469941845e-19, 1.4858416226630983e-18, 2.579968364443875e-18, 1.2503629419467144e-18, 0.01605525528284241, 1.941467513070613e-18, 1.5099364316481306e-18, 8.310252289239442e-19, 0.017732479119234625, 0.02153632231349602, 1.029635356642268e-18, 8.940560153363745e-19, 0.007968880810861613, 8.175443358239723e-19, 0.01654903700893719, 9.621339830151264e-19, 1.036480406981329e-18, 1.0726206672758613e-18, 4.758847901809915e-19, 0.01574419193746946, 0.016803733044812225, 9.529267474524037e-19, 1.0501241059347026e-18, 6.617187058571807e-19, 6.645231247110222e-19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
